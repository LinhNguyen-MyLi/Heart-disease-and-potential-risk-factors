---
title: "Heart disease"
author: "Linh"
date: "2023-09-15"
output: html_document
---


Each year, millions of individuals grapple with the onset of heart disease, making it a pervasive health concern both in the United States and globally. It stands as the leading cause of death among both men and women. Extensive statistical analyses have illuminated numerous risk factors linked to the development of heart disease. These factors encompass elements such as advancing age, blood pressure levels, total cholesterol levels, diabetes, hypertension, a family history predisposed to heart disease, obesity, and a sedentary lifestyle, among others. **In this notebook, we will delve into statistical tests and regression models using the Hungary and Switzerland heart disease dataset to investigate the correlation between one specific factor---maximum heart rate attainable during exercise---and its potential association with an increased likelihood of heart disease**

![](images/fitness.jpg)options(repos = "<https://cran.rstudio.com/>")

```{r}
# Set the CRAN mirror
options(repos = "https://cran.rstudio.com/")
```


```{r}

# Install necessary packages
install.packages("tidyverse")
install.packages("janitor")
install.packages("here")
install.packages("skimr")

library(tidyr)
library(dplyr)
library(readr)
library(ggplot2)
library(janitor)
library(here)
library(skimr)
library(dplyr)
library(sf)
```

```{r}
# Set the working directory to the folder where the CSV file is located
setwd("D:/STUDY/STUDY/R")

# Read the CSV file and store it in a data frame
Heart <- read.csv("Heart Disease Dataset Hungary - Switzerland.csv")
skim_without_charts(Heart)
Heart
```

```{r}
# load the tidyverse package
library(tidyverse)

# recode sex using mutate function and save as hd_data
Heart %>% mutate(sex = factor(sex, levels = 0:1, labels = c("Female", "Male")))-> Heart
Heart
```

Now, let's use statistical tests to see which predictors are related to heart disease. We can explore the associations for each variable in the dataset. Depending on the type of the data (i.e., continuous or categorical), we use t-test or chi-squared test to calculate the p-values.

Recall, t-test is used to determine whether there is a significant difference between the means of two groups (e.g., is the mean age from group A different from the mean age from group B?). A chi-squared test for independence compares the equivalence of two proportions.

```{r}
# Does sex have an effect? Sex is a binary variable in this dataset, so the appropriate test is chi-squared test
Heart_sex <- chisq.test(Heart$sex, Heart$target)

# Does age have an effect? Age is continuous, so we use t-test here
Heart_age <- t.test(Heart$age ~ Heart$target)

# What about thalach: maximum heart rate one can achieve during exercise?
Heart_rate <- t.test(Heart$thalach ~ Heart$target)

# Print the results to see if p<0.05.
print(Heart_sex)
print(Heart_age)
print(Heart_rate)
```

All p-value figures are smaller 0.05. In addition to p-values from statistical tests, we can plot the age, sex, and maximum heart rate distributions with respect to our outcome variable. This will give us a sense of both the direction and magnitude of the relationship. First, let's plot age using a boxplot since it is a continuous variable.

```{r}
# Recode target to be labelled
Heart %>% mutate(Heart_labelled = ifelse(target == 0, "No disease", "Disease")) -> Heart

# age vs hd
ggplot(data = Heart, aes(x = Heart_labelled, y = age)) + geom_boxplot()
```

Second, let's plot sex using a barplot since it is a binary variable in this dataset.

```{r}
# Define a custom color palette
my_colors <- c("Male" = "black", "Female" = "pink")

# Create the ggplot chart
ggplot(data = Heart, aes(x = Heart_labelled, fill = sex)) +
  geom_bar(position = "fill") +
  ylab("Sex %") +
  scale_fill_manual(values = my_colors)  # Apply the custom colors
```

```{r}
# max heart rate vs hd
ggplot(data = Heart, aes(x = Heart_labelled, y = thalach)) + geom_boxplot()
```

The plots and the statistical tests both confirmed that all the three variables are highly significantly associated with our outcome (p\<0.05 for all tests).

In general, we want to use multiple logistic regression when we have one binary outcome variable and two or more predicting variables. The binary variable is the dependent (Y) variable; we are studying the effect that the independent (X) variables have on the probability of obtaining a particular value of the dependent variable. For example, we might want to know the effect that maximum heart rate, age, and sex have on the probability that a person will have a heart disease in the next year. The model will also tell us what the remaining effect of maximum heart rate is after we control or adjust for the effects of the other two factors.

The glm() command is designed to perform generalized linear models (regressions) on binary outcome data, count data, probability data, proportion data, and many other data types. In our case, the outcome is binary following a binomial distribution.

```{r}
# use glm function from base R and specify the family argument as binomial
model <- glm(data = Heart, target ~ age + sex + thalach, family = "binomial" )

# extract the model summary
summary(model)
```

We use Odds Ratio (OR) to quantify how strongly the presence or absence of property A is associated with the presence or absence of the outcome. When the OR is greater than 1, we say A is positively associated with outcome B (increases the Odds of having B). Otherwise, we say A is negatively associated with B (decreases the Odds of having B).

The raw glm coefficient table (the 'estimate' column in the printed output) in R represents the log(Odds Ratios) of the outcome. Therefore, we need to convert the values to the original OR scale and calculate the corresponding 95% Confidence Interval (CI) of the estimated Odds Ratios when reporting results from a logistic regression.

```{r}
# load the broom package
library(broom)

# tidy up the coefficient table
tidy_m <- tidy(model)
tidy_m

# calculate OR
tidy_m$OR <- exp(tidy_m$estimate)

# calculate 95% CI and save as lower CI and upper CI
tidy_m$lower_CI <- exp(tidy_m$estimate - 1.96 * tidy_m$std.error)
tidy_m$upper_CI <- exp(tidy_m$estimate + 1.96 * tidy_m$std.error)

# display the updated coefficient table
tidy_m
```

Next, we'd like to translate the predicted probability into a decision rule for clinical use by defining a cutoff value on the probability scale. In practice, when an individual comes in for a health check-up, the doctor would like to know the predicted probability of heart disease, for specific values of the predictors: a 45-year-old female with a max heart rate of 150. To do that, we create a data frame called input_Heart, in which we include the desired values for our prediction.

```{r}
# get the predicted probability in our dataset using the predict() function
# We include the argument type=”response” in order to get our prediction.
pred_prob <- predict(model, Heart, type="response")

# create a decision rule using probability 0.5 as cutoff and save the predicted decision into the main data frame
Heart$pred_hd <- ifelse(pred_prob >= 0.5, 1, 0)

# create a newdata data frame to save a new case information
input_Heart <- data.frame(age=45, sex="Female", thalach=150)

# predict probability for this new case and print out the predicted value
p_new <- predict(model, input_Heart, type="response")
p_new
```

We are going to use some common metrics to evaluate the model performance. The most straightforward one is Accuracy, which is the proportion of the total number of predictions that were correct. On the other hand, we can calculate the classification error rate using 1- accuracy. However, accuracy can be misleading when the response is rare (i.e., imbalanced response). Another popular metric, Area Under the ROC curve (AUC), has the advantage that it's independent of the change in the proportion of responders. AUC ranges from 0 to 1. The closer it gets to 1 the better the model performance. Lastly, a confusion matrix is an N X N matrix, where N is the level of outcome. For the problem at hand, we have N=2, and hence we get a 2 X 2 matrix. It cross-tabulates the predicted outcome levels against the true outcome levels.

After these metrics are calculated, we'll see (from the logistic regression OR table) that older age, being male and having a lower max heart rate are all risk factors for heart disease. We can also apply our model to predict the probability of having heart disease. For a 45 years old female who has a max heart rate of 150, our model generated a heart disease probability of 0.177 indicating low risk of heart disease. Although our model has an overall accuracy of 0.71, there are cases that were misclassified as shown in the confusion matrix.

```{r}
# load Metrics package
library(Metrics)

# calculate auc, accuracy, clasification error
auc <- auc(Heart$target, Heart$pred_hd) 
accuracy <- accuracy(Heart$target, Heart$pred_hd)
classification_error <- ce(Heart$target, Heart$pred_hd) 

# print out the metrics on to screen
print(paste("AUC=", auc))
print(paste("Accuracy=", accuracy))
print(paste("Classification Error=", classification_error))

# confusion matrix
table(Heart$pred_hd, Heart$pred_hd, dnn=c("True Status", "Predicted Status")) # confusion matrix
```
